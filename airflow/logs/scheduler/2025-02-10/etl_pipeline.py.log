[2025-02-09T18:47:23.029-0700] {processor.py:186} INFO - Started process (PID=18631) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:47:23.029-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:47:23.030-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:47:23.029-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:47:23.031-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:47:23.031-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:47:23.031-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:47:23.247-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.220 seconds
[2025-02-09T18:49:09.336-0700] {processor.py:186} INFO - Started process (PID=19028) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:49:09.336-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:49:09.337-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:49:09.337-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:49:09.337-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:49:09.337-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:49:09.338-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:49:09.608-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.274 seconds
[2025-02-09T18:50:21.613-0700] {processor.py:186} INFO - Started process (PID=19253) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:50:21.613-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:50:21.614-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:50:21.614-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:50:21.615-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:50:21.615-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:50:21.615-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:50:21.839-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.229 seconds
[2025-02-09T18:51:32.324-0700] {processor.py:186} INFO - Started process (PID=19472) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:51:32.325-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:51:32.325-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:51:32.325-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:51:32.326-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:51:32.326-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:51:32.326-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:51:32.535-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.212 seconds
[2025-02-09T18:52:31.559-0700] {processor.py:186} INFO - Started process (PID=19667) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:52:31.559-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:52:31.560-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:52:31.560-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:52:31.561-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:52:31.560-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:52:31.561-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:52:31.770-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.213 seconds
[2025-02-09T18:54:15.678-0700] {processor.py:186} INFO - Started process (PID=19988) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:54:15.680-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:54:15.680-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:54:15.680-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:54:15.681-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:54:15.681-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:54:15.682-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:54:15.904-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.242 seconds
[2025-02-09T18:55:27.264-0700] {processor.py:186} INFO - Started process (PID=20204) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:55:27.264-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:55:27.265-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:55:27.265-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:55:27.265-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:55:27.265-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:55:27.265-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:55:27.495-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.234 seconds
[2025-02-09T18:56:39.628-0700] {processor.py:186} INFO - Started process (PID=20440) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:56:39.628-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:56:39.629-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:56:39.629-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:56:39.629-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:56:39.629-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:56:39.630-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:56:39.851-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.225 seconds
[2025-02-09T18:57:52.445-0700] {processor.py:186} INFO - Started process (PID=20660) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:57:52.445-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T18:57:52.446-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:57:52.446-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:57:52.446-0700] {logging_mixin.py:190} INFO - [2025-02-09T18:57:52.446-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T18:57:52.447-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T18:57:52.670-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.228 seconds
[2025-02-09T19:06:08.842-0700] {processor.py:186} INFO - Started process (PID=21844) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:06:08.843-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:06:08.843-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:06:08.843-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:06:08.844-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:06:08.844-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T19:06:08.844-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:06:09.066-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.227 seconds
[2025-02-09T19:07:47.802-0700] {processor.py:186} INFO - Started process (PID=22153) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:07:47.802-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:07:47.803-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:07:47.803-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:07:47.804-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:07:47.803-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T19:07:47.804-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:07:48.015-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.215 seconds
[2025-02-09T19:08:58.703-0700] {processor.py:186} INFO - Started process (PID=22376) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:08:58.704-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:08:58.704-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:08:58.704-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:08:58.705-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:08:58.705-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T19:08:58.705-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:08:58.914-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.212 seconds
[2025-02-09T19:10:10.412-0700] {processor.py:186} INFO - Started process (PID=22598) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:10:10.412-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:10:10.413-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:10:10.413-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:10:10.413-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:10:10.413-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 2, in <module>
    from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator, S3FileTransformOperator
ModuleNotFoundError: No module named 'airflow.providers.amazon'
[2025-02-09T19:10:10.413-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:10:10.631-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.221 seconds
[2025-02-09T19:12:37.420-0700] {processor.py:186} INFO - Started process (PID=23001) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:12:37.421-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:12:37.421-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:12:37.421-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:12:37.487-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:12:37.485-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:12:37.487-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:12:37.698-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.279 seconds
[2025-02-09T19:14:22.337-0700] {processor.py:186} INFO - Started process (PID=23321) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:14:22.337-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:14:22.338-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:14:22.337-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:14:22.404-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:14:22.403-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:14:22.404-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:14:22.621-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.286 seconds
[2025-02-09T19:15:31.178-0700] {processor.py:186} INFO - Started process (PID=23535) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:15:31.179-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:15:31.179-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:15:31.179-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:15:31.244-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:15:31.244-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:15:31.245-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:15:31.455-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.278 seconds
[2025-02-09T19:16:40.879-0700] {processor.py:186} INFO - Started process (PID=23749) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:16:40.881-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:16:40.881-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:16:40.881-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:16:40.954-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:16:40.952-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:16:40.954-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:16:41.210-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.332 seconds
[2025-02-09T19:17:53.777-0700] {processor.py:186} INFO - Started process (PID=23968) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:17:53.777-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:17:53.778-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:17:53.778-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:17:53.845-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:17:53.845-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:17:53.845-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:17:54.073-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.298 seconds
[2025-02-09T19:19:07.015-0700] {processor.py:186} INFO - Started process (PID=24197) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:19:07.016-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:19:07.016-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:19:07.016-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:19:07.081-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:19:07.081-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:19:07.082-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:19:07.295-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.281 seconds
[2025-02-09T19:20:19.666-0700] {processor.py:186} INFO - Started process (PID=24419) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:20:19.667-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:20:19.667-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:20:19.667-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:20:19.738-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:20:19.737-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:20:19.738-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:20:19.966-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.301 seconds
[2025-02-09T19:21:33.382-0700] {processor.py:186} INFO - Started process (PID=24638) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:21:33.383-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:21:33.384-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:21:33.383-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:21:33.449-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:21:33.448-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:21:33.449-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:21:33.660-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.279 seconds
[2025-02-09T19:22:47.274-0700] {processor.py:186} INFO - Started process (PID=24957) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:22:47.275-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:22:47.275-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:22:47.275-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:22:47.340-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:22:47.340-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:22:47.340-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:22:47.560-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.287 seconds
[2025-02-09T19:23:56.393-0700] {processor.py:186} INFO - Started process (PID=25174) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:23:56.393-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:23:56.394-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:23:56.394-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:23:56.459-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:23:56.459-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:23:56.460-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:23:56.672-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.281 seconds
[2025-02-09T19:25:02.417-0700] {processor.py:186} INFO - Started process (PID=25376) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:25:02.417-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:25:02.418-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:25:02.418-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:25:02.483-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:25:02.483-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:25:02.483-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:25:02.695-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.279 seconds
[2025-02-09T19:26:12.016-0700] {processor.py:186} INFO - Started process (PID=25605) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:26:12.016-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:26:12.017-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:26:12.017-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:26:12.081-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:26:12.081-0700] {dagbag.py:387} ERROR - Failed to import: /Users/pranithjain/airflow/dags/etl_pipeline.py
Traceback (most recent call last):
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranithjain/airflow/dags/etl_pipeline.py", line 55, in <module>
    upload_to_s3_task = S3FileTransformOperator(
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py", line 667, in __init__
    super().__init__(**kwargs)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 506, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/Users/pranithjain/Desktop/DE Project/Apache Airflow Project/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 957, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to S3FileTransformOperator (task_id: upload_transformed_data). Invalid arguments were:
**kwargs: {'aws_conn_id': 'aws_default'}
[2025-02-09T19:26:12.082-0700] {processor.py:927} WARNING - No viable dags retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:26:12.299-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.285 seconds
[2025-02-09T19:27:23.875-0700] {processor.py:186} INFO - Started process (PID=25938) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:27:23.876-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:27:23.877-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:23.877-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:27:23.951-0700] {processor.py:925} INFO - DAG(s) 'etl_pipeline_s3_redshift' retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:27:24.034-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.034-0700] {override.py:1930} INFO - Created Permission View: can read on DAG:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.038-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.038-0700] {override.py:1930} INFO - Created Permission View: can delete on DAG:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.041-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.041-0700] {override.py:1930} INFO - Created Permission View: can edit on DAG:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.044-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.044-0700] {override.py:1930} INFO - Created Permission View: can read on DAG Run:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.046-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.046-0700] {override.py:1930} INFO - Created Permission View: can create on DAG Run:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.049-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.048-0700] {override.py:1930} INFO - Created Permission View: can delete on DAG Run:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.052-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.051-0700] {override.py:1930} INFO - Created Permission View: menu access on DAG Run:etl_pipeline_s3_redshift
[2025-02-09T19:27:24.052-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.052-0700] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-09T19:27:24.058-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.057-0700] {dag.py:3262} INFO - Creating ORM DAG for etl_pipeline_s3_redshift
[2025-02-09T19:27:24.061-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:27:24.061-0700] {dag.py:4180} INFO - Setting next_dagrun for etl_pipeline_s3_redshift to 2025-02-09 00:00:00+00:00, run_after=2025-02-10 00:00:00+00:00
[2025-02-09T19:27:24.068-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.195 seconds
[2025-02-09T19:28:02.334-0700] {processor.py:186} INFO - Started process (PID=26053) to work on /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:28:02.334-0700] {processor.py:914} INFO - Processing file /Users/pranithjain/airflow/dags/etl_pipeline.py for tasks to queue
[2025-02-09T19:28:02.335-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:28:02.335-0700] {dagbag.py:588} INFO - Filling up the DagBag from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:28:02.400-0700] {processor.py:925} INFO - DAG(s) 'etl_pipeline_s3_redshift' retrieved from /Users/pranithjain/airflow/dags/etl_pipeline.py
[2025-02-09T19:28:02.440-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:28:02.440-0700] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-09T19:28:02.448-0700] {logging_mixin.py:190} INFO - [2025-02-09T19:28:02.448-0700] {dag.py:4180} INFO - Setting next_dagrun for etl_pipeline_s3_redshift to 2025-02-09 00:00:00+00:00, run_after=2025-02-10 00:00:00+00:00
[2025-02-09T19:28:02.454-0700] {processor.py:208} INFO - Processing /Users/pranithjain/airflow/dags/etl_pipeline.py took 0.123 seconds
